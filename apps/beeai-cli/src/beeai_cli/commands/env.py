# Copyright 2025 IBM Corp.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import os
import sys
import tempfile
import time
from beeai_cli.commands.provider import list_providers
import typer
import httpx
import subprocess
from rich.table import Column
from InquirerPy import inquirer
from InquirerPy.base.control import Choice
from InquirerPy.validator import EmptyInputValidator

from beeai_cli.api import api_request
from beeai_cli.async_typer import AsyncTyper, console, err_console, create_table
from beeai_cli.utils import parse_env_var

app = AsyncTyper()


@app.command("add")
async def add(
    env: list[str] = typer.Argument(help="Environment variables to pass to provider"),
) -> None:
    """Store environment variables"""
    env_vars = [parse_env_var(var) for var in env]
    env_vars = {name: value for name, value in env_vars}
    await api_request(
        "put",
        "env",
        json={**({"env": env_vars} if env_vars else {})},
    )
    await list_env()


@app.command("list")
async def list_env():
    """List stored environment variables"""
    # TODO: extract server schemas to a separate package
    resp = await api_request("get", "env")
    with create_table(Column("name", style="yellow"), "value") as table:
        for name, value in sorted(resp["env"].items()):
            table.add_row(name, value)
    console.print(table)


@app.command("remove")
async def remove_env(
    env: list[str] = typer.Argument(help="Environment variable(s) to remove"),
):
    await api_request("put", "env", json={**({"env": {var: None for var in env}})})
    await list_env()


@app.command(
    "sync", help="Sync external changes to provider registry (if you modified ~/.beeai/providers.yaml manually)"
)
async def sync():
    """Sync external changes to env configuration (if you modified ~/.beeai/.env manually)"""
    await api_request("put", "env/sync")
    console.print("Env updated")


@app.command("setup", help="Interactive setup for LLM provider environment variables")
async def setup() -> bool:
    """Interactive setup for LLM provider environment variables"""
    provider_name, api_base, recommended_model = await inquirer.select(
        message="Select LLM provider:",
        choices=[
            Choice(
                name="OpenAI".ljust(20) + "üöÄ best performance", value=("OpenAI", "https://api.openai.com/v1", "gpt-4o")
            ),
            Choice(
                name="DeepSeek".ljust(20) + "üöÄ best performance",
                value=("DeepSeek", "https://api.deepseek.com/v1", "deepseek-reasoner"),
            ),
            Choice(
                name="Groq".ljust(20) + "üÜì has a free tier",
                value=("Groq", "https://api.groq.com/openai/v1", "deepseek-r1-distill-llama-70b"),
            ),
            Choice(
                name="OpenRouter".ljust(20) + "üÜì has some free models",
                value=("OpenRouter", "https://openrouter.ai/api/v1", "deepseek/deepseek-r1-distill-llama-70b:free"),
            ),
            Choice(
                name="Anthropic Claude".ljust(20) + "üöß experimental",
                value=("Anthropic", "https://api.anthropic.com/v1", "claude-3-7-sonnet-20250219"),
            ),
            Choice(
                name="NVIDIA NIM".ljust(20) + "üöß experimental",
                value=("NVIDIA NIM", "https://integrate.api.nvidia.com/v1", "deepseek-ai/deepseek-r1"),
            ),
            Choice(name="Mistral".ljust(20) + "üöß experimental", value=("Mistral", "https://api.mistral.ai/v1", None)),
            # Choice(
            #     name="Cohere".ljust(20) + "üöß experimental",
            #     value=("Cohere", "https://api.cohere.ai/compatibility/v1", "command-r-plus"),
            # ),
            Choice(
                name="Perplexity".ljust(20) + "üöß experimental", value=("Perplexity", "https://api.perplexity.ai", None)
            ),
            Choice(name="Ollama".ljust(20) + "üíª local", value=("Ollama", "http://localhost:11434/v1", "llama3.1:8b")),
            Choice(name="Jan".ljust(20) + "üíª local", value=("Jan", "http://localhost:1337/v1", None)),
            Choice(name="Other".ljust(20) + "‚öôÔ∏è  provide API URL", value=("Other", None, None)),
        ],
    ).execute_async()

    if provider_name == "Other":
        api_base = await inquirer.text(
            message="Enter the base URL of your API (OpenAI-compatible):",
            validate=lambda url: (url.startswith(("http://", "https://")) or "URL must start with http:// or https://"),
            transformer=lambda url: url.rstrip("/"),
        ).execute_async()

    api_key = (
        "dummy"
        if provider_name in ["Ollama", "Jan"]
        else await inquirer.secret(message="Enter API key:", validate=EmptyInputValidator()).execute_async()
    )

    try:
        with console.status("Loading available models...", spinner="dots"):
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"{api_base}/models", headers={"Authorization": f"Bearer {api_key}"}, timeout=10.0
                )
                if response.status_code == 404:
                    available_models = []
                else:
                    response.raise_for_status()
                    available_models = [m.get("id", "") for m in response.json().get("data", [])]
    except httpx.HTTPStatusError:
        console.print("üí• [bold red]Error:[/bold red] API key was rejected. Please check your API key and re-try.")
        return False
    except httpx.HTTPError as e:
        console.print(f"üí• [bold red]Error:[/bold red] {str(e)}")
        match provider_name:
            case "Ollama":
                console.print("üí° [yellow]HINT[/yellow]: We could not connect to Ollama. Is it running?")
            case "Jan":
                console.print(
                    "üí° [yellow]HINT[/yellow]: We could not connect to Jan. Ensure that the server is running: in the Jan application, click the [bold][<>][/bold] button and [bold]Start server[/bold]."
                )
            case "Other":
                console.print(
                    "üí° [yellow]HINT[/yellow]: We could not connect to the API URL you have specified. Is it correct?"
                )
            case _:
                console.print(f"üí° [yellow]HINT[/yellow]: {provider_name} may be down.")
        return False

    if provider_name == "Ollama":
        available_models = [model for model in available_models if not model.endswith("-beeai")]

    selected_model = (
        recommended_model
        if (
            (not available_models or recommended_model in available_models or provider_name == "Ollama")
            and await inquirer.confirm(
                message=f"Do you want to use the recommended model '{recommended_model}'?"
                + (
                    " It will be pulled from Ollama now."
                    if recommended_model not in available_models and provider_name == "Ollama"
                    else ""
                ),
                default=True,
            ).execute_async()
        )
        else (
            await inquirer.fuzzy(
                message="Select a model (type to filter):",
                choices=sorted(available_models),
            ).execute_async()
            if available_models
            else await inquirer.text(message="Write a model name to use:").execute_async()
        )
    )

    if provider_name == "Ollama" and selected_model not in available_models:
        try:
            subprocess.run(["ollama", "pull", selected_model], check=True)
        except Exception as e:
            console.print(f"[red]Error while pulling model: {str(e)}[/red]")
            return False

    if provider_name == "Ollama" and (
        (
            num_ctx := await inquirer.select(
                message="Larger context window helps agents see more information at once at the cost of memory consumption, as long as the model supports it. Set a larger context window?",
                choices=[
                    Choice(name="2k  ‚ö†Ô∏è  some agents won't work", value=2048),
                    Choice(name="4k  ‚ö†Ô∏è  some agents won't work", value=4096),
                    Choice(name="8k", value=8192),
                    Choice(name="16k", value=16384),
                    Choice(name="32k", value=32768),
                    Choice(name="64k", value=65536),
                    Choice(name="128k", value=131072),
                ],
            ).execute_async()
        )
        > 2048
    ):
        modified_model = f"{recommended_model}-beeai"
        console.print(
            f"‚ö†Ô∏è  [yellow]Warning[/yellow]: BeeAI will create and use a modified version of this model tagged [bold]{modified_model}[/bold] with default context window set to [bold]{num_ctx}[/bold]."
        )

        try:
            if modified_model in available_models:
                subprocess.run(["ollama", "rm", modified_model], check=False)
            with tempfile.TemporaryDirectory() as temp_dir:
                modelfile_path = os.path.join(temp_dir, "Modelfile")
                with open(modelfile_path, "w") as f:
                    f.write(f"FROM {selected_model}\n\nPARAMETER num_ctx {num_ctx}\n")
                subprocess.run(["ollama", "create", modified_model], cwd=temp_dir, check=True)
        except Exception as e:
            console.print(f"[red]Error setting up Ollama model: {str(e)}[/red]")
            return False

        selected_model = modified_model

    try:
        with console.status("Checking if the model works...", spinner="dots"):
            async with httpx.AsyncClient() as client:
                test_response = await client.post(
                    f"{api_base}/chat/completions",
                    json={
                        "model": selected_model,
                        "max_tokens": 500,  # reasoning models need some tokens to think about this
                        "messages": [
                            {
                                "role": "system",
                                "content": "Repeat each message back to the user, verbatim. Don't say anything else.",
                            },
                            {"role": "user", "content": "Hello!"},
                        ],
                    },
                    headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                    timeout=10.0,
                )
        test_response.raise_for_status()
        response_text = test_response.json().get("choices", [{}])[0].get("message", {}).get("content", "")
        if "Hello!" not in response_text:
            console.print(
                f"[bold red]Model did not provide a proper response. The response:[/bold red] {response_text}"
            )
            return False
    except Exception as e:
        console.print(f"[bold red]Error during model test: {str(e)}[/bold red]")
        return False

    with console.status("Saving configuration...", spinner="dots"):
        await api_request(
            "put",
            "env",
            json={"env": {"LLM_API_BASE": api_base, "LLM_API_KEY": api_key, "LLM_MODEL": selected_model}},
        )

    with console.status("Reloading agent providers...", spinner="dots"):
        time.sleep(5)
        for i in range(30):
            time.sleep(1)
            if all(item["status"] == "ready" for item in (await api_request("get", "provider"))["items"]):
                break
        else:
            console.print(
                "[bold red]Some providers did not properly start. Please check their status with:[/bold red] beeai provider info <provider>"
            )
            await list_providers()

    console.print(
        "\n[bold green]You're all set![/bold green] (You can re-run this setup anytime with [blue]beeai env setup[/blue])"
    )
    return True


async def ensure_llm_env():
    try:
        env = (await api_request("get", "env"))["env"]
    except httpx.HTTPStatusError:
        return  # Skip for non-conforming servers (like when running directly against an agent provider)
    if all(required_variable in env.keys() for required_variable in ["LLM_MODEL", "LLM_API_KEY", "LLM_API_BASE"]):
        return
    console.print("[bold]Welcome to üêù [red]BeeAI[/red]![/bold]")
    console.print("Let's start by configuring your LLM environment.\n")
    if not await setup():
        err_console.print(
            ":boom: [bold red]Error[/bold red]: Could not continue because the LLM environment is not properly set up."
        )
        err_console.print(
            "üí° [yellow]HINT[/yellow]: Try re-entering your LLM API details with: [green]beeai env setup[/green]"
        )
        sys.exit(1)
    console.print()
